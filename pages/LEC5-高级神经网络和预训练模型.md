- ![Lecture5-高级神经网络和预训练模型.pdf](../assets/Lecture5-高级神经网络和预训练模型_1679278338397_0.pdf)
  title:: LEC5-高级神经网络和预训练模型
- # Attention机制
	- [[机器翻译]]（Machine Translation）是一个将源语言的句子x翻译成目标语言句子 y的任务。
	- [[神经机器翻译]]
		- [[Neural Machine Translation]]
		- 用[[端到端]]（[[end-to-end]]）的神经网络来求解机器翻译任务。
	- [[编码器-解码器框架]]
	  tags:: Encoder-decoder
		- [[Seq2Seq]]
		- [[编码器]]: 用来编码源语言的输入
		- [[解码器]]: 用来生成目标语言的输出
		- ((6417c1f7-dc35-4267-bfb1-a47dbe057945))
		  tags:: 解码器, 编码器, RNN
			- 通常情况下, [[解码器]]和[[编码器]]的RNN核不是同一个, 但通过一些方法可以使其共享参数, 更好地符合原义.
			- 解码器RNN核接受全部输入后的状态作为编码器的起始状态.
			- 编码器的前一个输出作为后一步的输入
		- ((6417c216-7ce4-468f-97fd-a27440da44a3))
		  tags:: 训练, 编码器-解码器框架, 端到端训练, Teacher-forcing
			- 训练: 输入中文, 目标为对应的英文, 计算每一步输出(应该为概率分布词表)的损失.
		- 通用性-一般的 [[Seq2Seq]]的任务都可以使用:
			- [[文本摘要]]: 长文本 to 短文本
			- [[对话生成]]: 之前的对话 to 下一句对话
			- [[代码生成]]: 自然语言 to 编程代码
		- [[编码器-解码器框架的问题]]
			- [[编码器]]运行到最后, 承载的信息太多, 前面的信息可能占比很小了.
			- 不具有[[可解释性]].
			- 不能很好地建模序列中的==非线性结构关系==
			- 无法并行。 RNN隐层状态具有序列依赖性、时间消耗随序列长度增加而增加。
			- 解决方法: [[Attention]]
	- [[Attention]]
	  tags:: 注意力机制
		- ((6417c5c1-158b-4d86-97de-6ca5fef51c12))
		- 目标端解码时，==直接从源端句子捕获对当前解码有帮助的信息==，从而生成更相关、更更准确的解码结果
		- 优点:
			- 缓解RNN中的信息瓶颈问题
			- 缓解长距离依赖问题
			- 具有一定的可解释性
		- 图解:
			- ((6417c618-b6a8-40e4-b35d-d50503f71618))
			  tags:: Attention, 编码器-解码器框架
			- ((6417c641-6a76-4525-9e7a-a577d07ee4e1))
			  tags:: Softmax, 编码器-解码器框架, 注意力打分
			- ((6417c67d-0195-431c-8dd2-30129888548b))
			  tags:: 注意力输出, 注意力权重
				- 不同语言之间不是词对词的. 通过权重大小将不同词之间的对应关系表示出来.
			- ((6417c6f1-7252-4744-a634-06e7fbed17cb))
			- ((6417c700-64cd-4d51-9f6b-d527035a5129))
			- ((6417c84a-bbb6-4a84-8d61-5138f7c55034))
			  tags:: 注意力可视化
		- [[Attention计算]]
			- ((6417c73d-9500-4ee0-87f4-21e5d515bcc5))
			- 编码器的隐层状态为$h_1,h_2,...,h_N\in R^h$
			- t时刻, 解码器的隐层状态为$s_t$
			- 对于时刻, 编码器隐层状态的[[注意力打分]]为$e^t=[s_t^Th_1,...,s_t^Th_N]\in R^N$
			- 利用 [[Softmax]]函数将[[注意力打分]] 转换为概率化的[[注意力权重]]: $\alpha^t=softmax(e^t)\in R^N$
			- 利用[[注意力权重]]得到编码器隐层状态的加权输出: $a^t=\sum_{i=1}^N \alpha_i^t h_i\in R^h$
			- ((6417c863-6f34-4ab0-919f-f2cb99bee9cb))
			- [[注意力打分]]可用函数
				- [[向量内积]]: $e_i= S^T h_i$
				- [[双线性变换]]: $e_i = S^TWh_i, s\in R^s, h_i\in R^h, W\in R^{s\times h}$
				- [[感知机]]: $e_i=s^TW_1+h_i^TW_2, s\in R^s, h_i\in R^h, W_1\in R^s,W_2\in R^h$
	- [[文本分类模型]]中的[[注意力机制]]
		- ((6417c92b-7f21-4cdd-b9d1-5b5f2cf51797))
		- ((6417c948-76b4-40f9-858d-853dcd249646))
	- [[文本摘要]]中的[[注意力机制]]
		- ((6417c9cf-4ff1-4eba-8e45-e7313b04f380))
	- [[视觉问答]]中的[[注意力机制]]
		- ((6417c9e4-55bb-4ebc-9a5d-4cf9010aae31))
- # Transformer网络
	- [[RNN]]的问题
		- 有限的信息交互距离
			- RNN能捕捉局部信息，但无法很好地解决[[长距离依赖关系]]（long-distance dependency）
			- 不能很好地建模序列中的[[非线性结构关系]]
		- 无法并行
			- RNN的隐层状态具有[[序列依赖性]]
			- 时间消耗随序列长度的增加而增加30输入文本
	- [[Self Attention]]
		- ((6417cb3c-d1ab-4dad-9927-bf12ad179b7f))
	- [[Transformer]]
		- 完全基于[[attention]]机制构建的神经网络模型
		- 直接建模输入序列的==全局依赖关系==
		- 并行计算
		- [[Transformer总体架构]]
			- 一个基于 [[Attention]]的[[编码器]]
				- 1. [[输入编码]]+[[位置编码]]
				  2. [[多头注意力机制]]
				  3. [[残差连接]]&[[层正则]]
				  4. [[前馈神经网络]]
				  5. 残差练级&层正则
				- ((6417cbf9-2c95-4cda-949b-ce441bb135af))
			- ((6417cbc1-b304-4bc1-9c9c-cb59da8c5433))
			-
	- [[自注意力机制]]
		- 计算 [[Attention]]需要的queries, keys, values: $Q=XW^Q, K=XW^K, V=XW^V$
		- 根据queries核keys计算attention打分$E=QK^T$
		- 计算attention权重$A=softmax(\frac{E}{\sqrt{d_k}})$
		- 根据[[注意力权重]]A和values计算输出$O=AV$
		- [[多头自注意力机制]]
			- 并行地计算多个自注意力过程, 并拼接输出结果:$O=[O_1,O_2,...,O_M], O_i=softmax(\frac{Q_iK_i^T}{\sqrt{d_k}})V_i$
	- [[前馈神经网络]]
		- ((6417d214-28ea-4082-86b4-b3345c3e8ab7))
	- [[Transformer]]的[[解码器]]
		- [[Cross Attention]]: 解码时需要关注==源端信息==
		- [[Masked Attention]]: 训练解码器时==不应该看到未来的信息==
			- ((6417d270-57f2-4b7b-8a74-af02ed1ca0ac))
		- ((6417d359-071b-4265-a296-11f7f5b76fab))
	- ## 应用
		- ((6417d3a9-1ffb-4337-b93c-07c035ae889a))
		- ((646adf3e-38b7-4d9d-9ff4-56b35b5bf766))
		- ((6417d3b2-068c-4222-82e9-537a3b19eb49))
		- ((6417d3d0-5b51-48ab-b707-2297da093646))
		-
		-
- # 预训练模型
	- [[词向量]]的弊端
		- 词向量表示是[[上下文无关]]的，不会随着上下文的改变而改变. 对于[[上下文相关]]的此表示来说, 单词应该随着上下文的改变而改变.
		- 单词具有[[一词多义性]]，同一个词在不同上下文中可能具有不同语义
	- [[预训练模型]]
		- [[上下文相关]]的词表示，建模单词在具体上下文中的语义
		- 对于下游任务，避免从头训练模型. 新范式：先预训练（[[pre-training]]），后[[微调]]（[[fine-tuning]]）
		- [[ELMO]]
		- [[BERT]]
		-